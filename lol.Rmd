---
title: "CatBoost R tutorial"
output: html_notebook
---

```{r}
library(caret)
library(titanic)
library(catboost)
```
Load and preprocess the Titanic dataset.
```{r}
set.seed(12345)

data <- as.data.frame(as.matrix(titanic_train), stringsAsFactors=TRUE)

age_levels <- levels(data$Age)
most_frequent_age <- which.max(table(data$Age))
data$Age[is.na(data$Age)] <- age_levels[most_frequent_age]

drop_columns = c("PassengerId", "Survived", "Name", "Ticket", "Cabin")
x <- data[,!(names(data) %in% drop_columns)]
y <- data[,c("Survived")]
```
At training we use 5-fold cross-validation. Also try to find the optimal trees' depth.
```{r}
fit_control <- trainControl(method = "cv",
                            number = 5,
                            classProbs = TRUE)

grid <- expand.grid(depth = c(4, 6, 8),
                    learning_rate = 0.1,
                    iterations = 100,
                    l2_leaf_reg = 1e-3,
                    rsm = 0.95,
                    border_count = 64)

report <- train(x, as.factor(make.names(y)),
                method = catboost.caret,
                verbose = FALSE, preProc = NULL,
                tuneGrid = grid, trControl = fit_control)
```
And print the result.
```{r}
print(report)

importance <- varImp(report, scale = FALSE)
print(importance)
```
